{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe9eb9-2154-445e-acbc-6df6d36425d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c0aef-9a57-4b9d-a2f0-33a883f59dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafa1ee5-abeb-43f3-b17d-00cd6ae20b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5c2f3bd-1fff-447a-941b-dd6744749b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "from groq import AsyncGroq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "175c89e7-34ec-46a9-ba1f-7398035af36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    client = AsyncGroq()\n",
    "\n",
    "    stream = await client.chat.completions.create(\n",
    "        #\n",
    "        # Required parameters\n",
    "        #\n",
    "        messages=[\n",
    "            # Set an optional system message. This sets the behavior of the\n",
    "            # assistant and can be used to provide specific instructions for\n",
    "            # how it should behave throughout the conversation.\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"you are a helpful assistant.\"\n",
    "            },\n",
    "            # Set a user message for the assistant to respond to.\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Explain the importance of fast language models\",\n",
    "            }\n",
    "        ],\n",
    "\n",
    "        # The language model which will generate the completion.\n",
    "        model=\"llama3-8b-8192\",\n",
    "\n",
    "        #\n",
    "        # Optional parameters\n",
    "        #\n",
    "\n",
    "        # Controls randomness: lowering results in less random completions.\n",
    "        # As the temperature approaches zero, the model will become\n",
    "        # deterministic and repetitive.\n",
    "        temperature=0.5,\n",
    "\n",
    "        # The maximum number of tokens to generate. Requests can use up to\n",
    "        # 2048 tokens shared between prompt and completion.\n",
    "        max_tokens=1024,\n",
    "\n",
    "        # Controls diversity via nucleus sampling: 0.5 means half of all\n",
    "        # likelihood-weighted options are considered.\n",
    "        top_p=1,\n",
    "\n",
    "        # A stop sequence is a predefined or user-specified text string that\n",
    "        # signals an AI to stop generating content, ensuring its responses\n",
    "        # remain focused and concise. Examples include punctuation marks and\n",
    "        # markers like \"[end]\".\n",
    "        stop=None,\n",
    "\n",
    "        # If set, partial message deltas will be sent.\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    # Print the incremental deltas returned by the LLM.\n",
    "    async for chunk in stream:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61414994-4883-4de9-908e-a33491a879a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object main at 0x7ff333639dc0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e33178-3aa6-49d8-89a7-44a5b213032f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c2b22-d5ae-4a7b-bc7f-c0d9e9786ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fc0582-a50b-4121-9b5a-2b888c49dbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c43186-12d8-4983-98b0-d1897f26a11a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a277bd-4152-437b-8734-29f22c45ffa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (groq_fund_commentary)",
   "language": "python",
   "name": "groq_fund_commentary"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
